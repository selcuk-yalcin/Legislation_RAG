"""
RAGAS Evaluation Framework for Legislation RAG System

Bu script RAG sisteminin kalitesini Ã¶lÃ§er:
- Faithfulness (Sadakat): Cevap kaynaÄŸa ne kadar sadÄ±k?
- Answer Relevancy (Ä°lgililik): Cevap soruya ne kadar uygun?
- Context Precision (BaÄŸlam Hassasiyeti): DoÄŸru dÃ¶kÃ¼manlar mÄ± alÄ±ndÄ±?
- Context Recall (BaÄŸlam HatÄ±rlama): TÃ¼m ilgili bilgi bulundu mu?

KullanÄ±m:
    python ragas_evaluation.py
"""

import os
import json
from datetime import datetime
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')

# RAGAS imports
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_relevancy
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    print("âš ï¸  RAGAS kurulu deÄŸil. LÃ¼tfen yÃ¼kleyin: pip install ragas")
    RAGAS_AVAILABLE = False

# Local imports
from mongodb_vector_store import get_mongodb_vectorstore
from client import create_openrouter_client
from reranker import RerankerService
from rag_pipeline import RAGPipeline


class RAGEvaluator:
    """RAGAS-based RAG system evaluator"""
    
    def __init__(self, output_dir="./evaluation_results"):
        """
        Initialize evaluator
        
        Args:
            output_dir: Directory to save evaluation results
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize RAG components
        print("ğŸš€ RAG sistemini baÅŸlatÄ±yorum...")
        self.client = create_openrouter_client()
        self.vectorstore = get_mongodb_vectorstore()
        self.reranker = RerankerService()
        self.rag_pipeline = RAGPipeline(self.client, self.vectorstore, self.reranker)
        print("âœ… RAG sistemi hazÄ±r!\n")
    
    def create_test_dataset(self) -> List[Dict]:
        """
        Create test dataset with questions and ground truth
        
        Returns:
            List of test cases
        """
        # Test sorularÄ± (gerÃ§ek kullanÄ±m senaryolarÄ±)
        test_cases = [
            {
                "question": "Ä°ÅŸverenin iÅŸ saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸi konusundaki yÃ¼kÃ¼mlÃ¼lÃ¼kleri nelerdir?",
                "ground_truth": "Ä°ÅŸveren, Ã§alÄ±ÅŸanlarÄ±n iÅŸ saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸini saÄŸlamakla yÃ¼kÃ¼mlÃ¼dÃ¼r. Risk deÄŸerlendirmesi yapmak, gerekli Ã¶nlemleri almak, Ã§alÄ±ÅŸanlarÄ± bilgilendirmek ve eÄŸitmek zorundadÄ±r."
            },
            {
                "question": "Risk deÄŸerlendirmesi nedir ve nasÄ±l yapÄ±lÄ±r?",
                "ground_truth": "Risk deÄŸerlendirmesi, iÅŸyerinde var olan ya da dÄ±ÅŸarÄ±dan gelebilecek tehlikelerin belirlenmesi, bu tehlikelerin riske dÃ¶nÃ¼ÅŸmesine yol aÃ§an faktÃ¶rler ile tehlikelerden kaynaklanan risklerin analiz edilerek derecelendirilmesi ve kontrol tedbirlerinin kararlaÅŸtÄ±rÄ±lmasÄ± Ã§alÄ±ÅŸmalarÄ±dÄ±r."
            },
            {
                "question": "Ä°ÅŸ gÃ¼venliÄŸi uzmanÄ± gÃ¶revlendirmesi zorunlu mudur?",
                "ground_truth": "Ä°ÅŸveren, iÅŸyerlerinde iÅŸ saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸi hizmetlerini yÃ¼rÃ¼tmek Ã¼zere iÅŸ gÃ¼venliÄŸi uzmanÄ± gÃ¶revlendirmek zorundadÄ±r. Bu zorunluluk iÅŸyerinin tehlike sÄ±nÄ±fÄ±na ve Ã§alÄ±ÅŸan sayÄ±sÄ±na gÃ¶re deÄŸiÅŸir."
            },
            {
                "question": "Ã‡alÄ±ÅŸan temsilcisi kimdir ve nasÄ±l seÃ§ilir?",
                "ground_truth": "Ã‡alÄ±ÅŸan temsilcisi, iÅŸ saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸi konularÄ±nda iÅŸveren ile Ã§alÄ±ÅŸanlar arasÄ±nda koordinasyonu saÄŸlayan, Ã§alÄ±ÅŸanlar tarafÄ±ndan seÃ§ilen kiÅŸidir. En az elli Ã§alÄ±ÅŸanÄ± olan iÅŸyerlerinde Ã§alÄ±ÅŸan temsilcisi bulundurulur."
            },
            {
                "question": "KiÅŸisel koruyucu donanÄ±m kullanÄ±mÄ± zorunlu mudur?",
                "ground_truth": "Ä°ÅŸveren, Ã§alÄ±ÅŸma ortamÄ±nda saÄŸlÄ±k ve gÃ¼venlik risklerinin mÃ¼hendislik tedbirleri ve diÄŸer yÃ¶ntemlerle Ã¶nlenemediÄŸi veya tam olarak sÄ±nÄ±rlandÄ±rÄ±lamadÄ±ÄŸÄ± durumlarda uygun kiÅŸisel koruyucu donanÄ±mlarÄ± saÄŸlamak ve kullandÄ±rmak zorundadÄ±r."
            }
        ]
        
        return test_cases
    
    def run_evaluation(self, test_cases: List[Dict]) -> Dict:
        """
        Run RAGAS evaluation on test cases
        
        Args:
            test_cases: List of test questions and ground truths
            
        Returns:
            Evaluation results
        """
        if not RAGAS_AVAILABLE:
            print("âŒ RAGAS yÃ¼klÃ¼ deÄŸil!")
            return {}
        
        print("=" * 70)
        print("ğŸ§ª RAGAS Evaluation BaÅŸlÄ±yor")
        print("=" * 70)
        
        # Prepare data for RAGAS
        questions = []
        answers = []
        contexts = []
        ground_truths = []
        
        print(f"\nğŸ“ {len(test_cases)} test sorusu iÅŸleniyor...\n")
        
        for i, test_case in enumerate(test_cases, 1):
            question = test_case["question"]
            ground_truth = test_case["ground_truth"]
            
            print(f"[{i}/{len(test_cases)}] Soru: {question[:60]}...")
            
            # Get RAG response
            try:
                # Get answer from RAG pipeline
                full_response = self.rag_pipeline.generate_response(question)
                
                # Extract answer (remove sources section)
                answer = full_response.split("â•" * 70)[0].strip()
                
                # Get context from last retrieval
                # We need to access the documents used
                # For now, we'll retrieve again for context
                from query_expansion import expand_query
                search_query = expand_query(self.client, question)
                initial_docs = self.vectorstore.similarity_search(search_query, k=50)
                relevant_docs = self.reranker.rerank_documents(search_query, initial_docs)
                
                # Context is the retrieved documents
                context_list = [doc.page_content for doc in relevant_docs[:5]]
                
                questions.append(question)
                answers.append(answer)
                contexts.append(context_list)
                ground_truths.append(ground_truth)
                
                print(f"    âœ“ Cevap alÄ±ndÄ± ({len(answer)} karakter)")
                print(f"    âœ“ Context: {len(context_list)} dÃ¶kÃ¼man\n")
                
            except Exception as e:
                print(f"    âŒ Hata: {e}\n")
                continue
        
        # Create RAGAS dataset
        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths
        }
        
        dataset = Dataset.from_dict(data)
        
        print("\n" + "=" * 70)
        print("ğŸ“Š RAGAS Metrikleri HesaplanÄ±yor...")
        print("=" * 70)
        
        # Run evaluation
        try:
            result = evaluate(
                dataset,
                metrics=[
                    faithfulness,          # Sadakat: Cevap kaynaklara ne kadar sadÄ±k?
                    answer_relevancy,      # Ä°lgililik: Cevap soruya ne kadar uygun?
                    context_precision,     # Hassasiyet: DoÄŸru dÃ¶kÃ¼manlar mÄ± alÄ±ndÄ±?
                    context_recall,        # HatÄ±rlama: TÃ¼m ilgili bilgi bulundu mu?
                    context_relevancy      # BaÄŸlam Ä°lgililikliliÄŸi
                ]
            )
            
            return result
            
        except Exception as e:
            print(f"âŒ Evaluation hatasÄ±: {e}")
            return {}
    
    def save_results(self, results: Dict, test_cases: List[Dict]):
        """
        Save evaluation results to file
        
        Args:
            results: RAGAS evaluation results
            test_cases: Original test cases
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ragas_evaluation_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        # Prepare output
        output = {
            "timestamp": timestamp,
            "date": datetime.now().isoformat(),
            "num_test_cases": len(test_cases),
            "metrics": {
                "faithfulness": float(results.get("faithfulness", 0)),
                "answer_relevancy": float(results.get("answer_relevancy", 0)),
                "context_precision": float(results.get("context_precision", 0)),
                "context_recall": float(results.get("context_recall", 0)),
                "context_relevancy": float(results.get("context_relevancy", 0))
            },
            "test_cases": test_cases
        }
        
        # Save to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {filepath}")
        
        return filepath
    
    def print_report(self, results: Dict):
        """
        Print evaluation report
        
        Args:
            results: RAGAS evaluation results
        """
        print("\n" + "=" * 70)
        print("ğŸ“Š RAGAS EVALUATION RAPORU")
        print("=" * 70)
        
        metrics = {
            "Faithfulness (Sadakat)": results.get("faithfulness", 0),
            "Answer Relevancy (Ä°lgililik)": results.get("answer_relevancy", 0),
            "Context Precision (Hassasiyet)": results.get("context_precision", 0),
            "Context Recall (HatÄ±rlama)": results.get("context_recall", 0),
            "Context Relevancy (BaÄŸlam Ä°lgililkliliÄŸi)": results.get("context_relevancy", 0)
        }
        
        print("\nğŸ“ˆ Metrik SkorlarÄ± (0-1 arasÄ±, 1 en iyi):\n")
        
        for metric_name, score in metrics.items():
            # Visual bar
            bar_length = int(score * 40)
            bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
            
            # Rating
            if score >= 0.8:
                rating = "ğŸŸ¢ MÃ¼kemmel"
            elif score >= 0.6:
                rating = "ğŸŸ¡ Ä°yi"
            elif score >= 0.4:
                rating = "ğŸŸ  Orta"
            else:
                rating = "ğŸ”´ DÃ¼ÅŸÃ¼k"
            
            print(f"{metric_name:40s}: {score:.3f} {bar} {rating}")
        
        # Overall score
        avg_score = sum(metrics.values()) / len(metrics)
        print("\n" + "-" * 70)
        print(f"{'GENEL ORTALAMA':40s}: {avg_score:.3f}")
        print("-" * 70)
        
        # Interpretation
        print("\nğŸ’¡ Metrik AÃ§Ä±klamalarÄ±:\n")
        print("  â€¢ Faithfulness: CevabÄ±n kaynaklara ne kadar sadÄ±k olduÄŸu")
        print("  â€¢ Answer Relevancy: CevabÄ±n soruyla ne kadar ilgili olduÄŸu")
        print("  â€¢ Context Precision: AlÄ±nan dÃ¶kÃ¼manlarÄ±n ne kadar doÄŸru olduÄŸu")
        print("  â€¢ Context Recall: TÃ¼m ilgili bilginin bulunup bulunmadÄ±ÄŸÄ±")
        print("  â€¢ Context Relevancy: BaÄŸlamÄ±n soruyla ne kadar ilgili olduÄŸu")
        
        # Recommendations
        print("\nğŸ¯ Ã–neriler:\n")
        
        if metrics["Faithfulness (Sadakat)"] < 0.7:
            print("  âš ï¸  Faithfulness dÃ¼ÅŸÃ¼k - LLM hallucination yapÄ±yor olabilir")
            print("      â†’ Prompt'u daha kÄ±sÄ±tlayÄ±cÄ± hale getirin")
            print("      â†’ \"ONLY use the information provided\" talimatÄ±nÄ± gÃ¼Ã§lendirin")
        
        if metrics["Answer Relevancy (Ä°lgililik)"] < 0.7:
            print("  âš ï¸  Answer Relevancy dÃ¼ÅŸÃ¼k - Cevaplar konudan sapÄ±yor")
            print("      â†’ Query expansion stratejisini gÃ¶zden geÃ§irin")
            print("      â†’ LLM prompt'unu daha spesifik yapÄ±n")
        
        if metrics["Context Precision (Hassasiyet)"] < 0.7:
            print("  âš ï¸  Context Precision dÃ¼ÅŸÃ¼k - YanlÄ±ÅŸ dÃ¶kÃ¼manlar alÄ±nÄ±yor")
            print("      â†’ Reranker modelini iyileÅŸtirin")
            print("      â†’ Embedding modelini fine-tune edin")
        
        if metrics["Context Recall (HatÄ±rlama)"] < 0.7:
            print("  âš ï¸  Context Recall dÃ¼ÅŸÃ¼k - BazÄ± ilgili bilgiler kaÃ§Ä±rÄ±lÄ±yor")
            print("      â†’ INITIAL_RETRIEVAL_K deÄŸerini artÄ±rÄ±n")
            print("      â†’ Vector search parametrelerini optimize edin")
        
        print("\n" + "=" * 70)


def main():
    """Main evaluation function"""
    
    print("\n" + "â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 20 + "RAGAS EVALUATION SYSTEM" + " " * 25 + "â•‘")
    print("â•š" + "â•" * 68 + "â•\n")
    
    if not RAGAS_AVAILABLE:
        print("âŒ RAGAS kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil!")
        print("\nğŸ“¦ YÃ¼klemek iÃ§in:")
        print("   pip install ragas")
        return
    
    # Initialize evaluator
    evaluator = RAGEvaluator()
    
    # Create test dataset
    print("ğŸ“ Test dataset'i hazÄ±rlanÄ±yor...")
    test_cases = evaluator.create_test_dataset()
    print(f"âœ… {len(test_cases)} test sorusu hazÄ±r\n")
    
    # Run evaluation
    results = evaluator.run_evaluation(test_cases)
    
    if results:
        # Print report
        evaluator.print_report(results)
        
        # Save results
        evaluator.save_results(results, test_cases)
        
        print("\nâœ… Evaluation tamamlandÄ±!")
    else:
        print("\nâŒ Evaluation baÅŸarÄ±sÄ±z!")


if __name__ == "__main__":
    main()
